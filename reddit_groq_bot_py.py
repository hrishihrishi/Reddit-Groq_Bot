# -*- coding: utf-8 -*-
"""Reddit_Groq_Bot.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K-DcYX7M7AxdUfzSrOeNHQ3on0audjAb
"""

!pip install praw requests schedule python-dotenv

#  !pip install praw requests schedule python-dotenv
import praw
import schedule
import time
import requests
import logging

reddit = praw.Reddit(
    client_id = your_client_id ,
    client_secret= your_client_secret,
    username= your_username,
    password= your_password,
    user_agent= your_user_agent
)

subreddit_name = "test"
topic = input("On what topic do you want to generate the post? : ")

"""PLAYING WITH GROK FOR CONTENT AND COMMENT"""

'''generate_content function sends post request to groq api and returns the ai generated content as string'''

def generate_content(title):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {"Authorization": your_groq_api_key, "Content-Type": "application/json"}
    data = {"model":"llama-3.3-70b-versatile",
            "messages" : [{"role": "user", "content": f"Write an engaging article about {title} in 100 words."}],
            "max_tokens": 100,    # increase tokens as per your rate limit
            "stream":False}

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        response_json = response.json()

        # Attempt to extract content
        if "choices" in response_json:
            #print("checking extraction") # uncomment it to check if extraction works properly
            #print(response_json['usage'])  # uncomment it to check if usage
            if response_json["choices"][0].get("finish_reason") == "length":
                print("Content was truncated. Consider increasing max_tokens.")
            return response_json["choices"][0]["message"]["content"]
        else:
            return "No content generated"

    except requests.exceptions.RequestException as e:
        print(f"HTTP Error: {e}")
        return None
    except ValueError as e:
        print(f"JSON Parse Error: {e}")
        return None

'''generat_comment function sends post request to groq api and returns the ai generated content as string, the 'post' parameter is the title of newest post in 'test' subreddit'''


def generate_comment(post_title):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {"Authorization": your_groq_api_key, "Content-Type": "application/json"}
    data = {"model":"llama-3.3-70b-versatile",
            "messages" : [{"role": "user", "content": f"Write a funny comment on {post_title} in 100 words."}],
            "max_tokens": 100,
            "stream":False}

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        response_json = response.json()
        if "choices" in response_json:
            if response_json["choices"][0].get("finish_reason") == "length":
                print("Content was truncated. Consider increasing max_tokens.")
            return response_json["choices"][0]["message"]["content"]
        else:
            return "No content generated"
    except requests.exceptions.RequestException as e:
        print(f"HTTP Error: {e}")
        return None
    except ValueError as e:
        print(f"JSON Parse Error: {e}")
        return None

"""POSTING PART STARTS HERE"""

'''This part posts content to reddit. it does so by calling generate_coetent function which gets content from groq ai.'''


# Configure logging and posting

logging.basicConfig(filename="reddit_bot.log", level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def post_to_reddit_with_logging():
    content = generate_content(title=topic)
    if content:
        try:
            reddit.subreddit(subreddit_name).submit(title=topic, selftext=content)
            logging.info(f"Post {topic} submitted successfully!")
        except Exception as e:
            logging.error(f"Error posting to Reddit: {e}")

    else:
        logging.warning("No content generated for the post.")

post_to_reddit_with_logging()

'''Find recent posts in a subreddit and comment on them. here the comment on post function extracts the newest post's title and send it as a parameter to generate_comment function'''

def comment_on_posts():
    subreddit_name = "test"  # Replace with your target subreddit
    try:
        subreddit = reddit.subreddit(subreddit_name)
        for submission in subreddit.new(limit=5):  # Fetch the 5 most recent posts
            if not submission.stickied:  # Avoid stickied posts
                comment = generate_comment(submission.title)
                if comment:
                    try:
                        submission.reply(comment)
                        logging.info(f"Commented on post: {submission.title}")
                    except Exception as e:
                        logging.error(f"Error commenting on post {submission.id}: {e}")
    except Exception as e:
        logging.error(f"Error accessing subreddit {subreddit_name}: {e}")

"""SCHEDULING IS HERE"""

# THIS SENDS POST EVERY X HOURS JUST CHANGE IF YOU WANT

schedule.every().day.at("14:23").do(post_to_reddit_with_logging)  # Change "09:00" to your preferred time
schedule.every().day.at("14:24").do(comment_on_posts)
while True:
    schedule.run_pending()
    time.sleep(1)